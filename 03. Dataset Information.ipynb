{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 03. Dataset Information\n",
    "Go to `pycocoImageEmbedding` and find out how many corrupt images are present, and remove their captions from the above pickle before moving on.\n",
    "\n",
    "Here, we're using the `corruption_free_coco_descriptions.pkl` to generate stats about our datasets.\n",
    "\n",
    "We find these variables:\n",
    "1. `VOCAB_SIZE`\n",
    "2. `UNIQUE WORDS` (this is input in place of `VOCAB_SIZE` in `final_model.ipynb`)\n",
    "3. `Total Captions`\n",
    "4. `MAX_LENGTH` (Maximum caption length)\n",
    "\n",
    "We also load glove vectors and generate **`embedding_matrix.pkl`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pickle\n",
    "\n",
    "from nltk import FreqDist\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_descriptions(file_path):\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_desc = load_descriptions(\"./corruption_free_coco_descriptions.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "82782\n"
     ]
    }
   ],
   "source": [
    "print(type(new_desc))\n",
    "print(len(new_desc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['startseq a restaurant has modern wooden tables and chairs  endseq', 'startseq a long restaurant table with rattan rounded back chairs  endseq', 'startseq a long table with a plant on top of it surrounded with wooden chairs  endseq', 'startseq a long table with a flower arrangement in the middle for meetings endseq', 'startseq a table is adorned with wooden chairs with blue accents  endseq']\n"
     ]
    }
   ],
   "source": [
    "for k, v in new_desc.items():\n",
    "    print(v)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Corpus in 0.30s\n",
      "The size of vocabulary is 23124\n"
     ]
    }
   ],
   "source": [
    "corpus = \"\"\n",
    "start = time.time()\n",
    "for ec in new_desc.values():\n",
    "    for el in ec:\n",
    "        corpus += \" \"+el\n",
    "print(\"Generated Corpus in {:.2f}s\".format(time.time() - start))\n",
    "\n",
    "total_words = corpus.split()\n",
    "vocabulary = set(total_words)\n",
    "print(\"The size of vocabulary is {}\".format(len(vocabulary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 684603),\n",
       " ('startseq', 414108),\n",
       " ('endseq', 414108),\n",
       " ('on', 150689),\n",
       " ('of', 142762)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating frequency distribution of words\n",
    "freq_dist = FreqDist(total_words)\n",
    "freq_dist.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing least common words from vocabulary\n",
    "for ew in list(vocabulary):\n",
    "    if(freq_dist[ew]<10):\n",
    "        vocabulary.remove(ew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique words after removing less frequent word from our corpus = 6321\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = len(vocabulary)+1\n",
    "print(\"Total unique words after removing less frequent word from our corpus = {}\".format(VOCAB_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total caption present = 414108\n"
     ]
    }
   ],
   "source": [
    "caption_list = []\n",
    "for el in new_desc.values():\n",
    "    for ec in el:\n",
    "        caption_list.append(ec)\n",
    "print(\"The total caption present = {}\".format(len(caption_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = Tokenizer(num_words=VOCAB_SIZE)\n",
    "token.fit_on_texts(caption_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index to words are assigned according to frequency. i.e the most frequent word has index of 1\n",
    "ix_to_word = token.index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in list(ix_to_word):\n",
    "    if k>=6321:\n",
    "        ix_to_word.pop(k, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_ix = dict()\n",
    "for k,v in ix_to_word.items():\n",
    "    word_to_ix[v] = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6320\n",
      "6320\n"
     ]
    }
   ],
   "source": [
    "print(len(word_to_ix))\n",
    "print(len(ix_to_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum caption has length of 52\n"
     ]
    }
   ],
   "source": [
    "# finding the max_length caption\n",
    "MAX_LENGTH = 0\n",
    "temp = 0\n",
    "for ec in caption_list:\n",
    "    temp = len(ec.split())\n",
    "    if(MAX_LENGTH<=temp):\n",
    "        MAX_LENGTH = temp\n",
    "\n",
    "print(\"Maximum caption has length of {}\".format(MAX_LENGTH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Glove Vectors file\n",
    "using 300 dimensions glove file since the article we're following uses a embedding size of 300\n",
    "\n",
    "Download pre-trained glove_vectors from [this link](https://nlp.stanford.edu/projects/glove/)\n",
    "\n",
    "We'll load the text file (`glove.6B.300d.txt`) and save it as a pickle (`glove_vectors.pkl`) to save us time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_glove_pickle(file_path):\n",
    "    embeddings_index = {}\n",
    "    start = time.time()\n",
    "    with open(file_path, encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "    print(\"Created embeddings_index in {:.2f}s\".format(time.time() - start))\n",
    "    \n",
    "    print(\"Saving embeddings_index as glove_vectors.pkl\")\n",
    "    start = time.time()\n",
    "    with open(\"glove_vectors.pkl\", \"wb\") as f:\n",
    "        pickle.dump(embeddings_index, f)\n",
    "    print(\"Saved glove_vectors in {:.2f}s\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created embeddings_index in 41.93s\n",
      "Saving embeddings_index as glove_vectors.pkl\n",
      "Saved glove_vectors in 45.51s\n"
     ]
    }
   ],
   "source": [
    "save_glove_pickle(\"glove/glove.6B.300d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_vectors(file_path):\n",
    "    start = time.time()\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        glove = pickle.load(f)\n",
    "        glove_words = set(glove.keys())\n",
    "    print(\"Loaded {} in {:.2f}s\".format(file_path, time.time() - start))\n",
    "    return glove, glove_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded glove_vectors.pkl in 2.45s\n"
     ]
    }
   ],
   "source": [
    "glove, glove_words = load_glove_vectors(\"glove_vectors.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(glove_words) # 400k\n",
    "# type(glove_words) # set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embedding_matrix in 0.03\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_SIZE = 300\n",
    "\n",
    "# Get 300-dim dense vector for each of the words in vocabulary\n",
    "embedding_matrix = np.zeros((VOCAB_SIZE,EMBEDDING_SIZE))\n",
    "embedding_matrix.shape\n",
    "\n",
    "start = time.time()\n",
    "for word, i in word_to_ix.items():\n",
    "    embedding_vector = np.zeros(300)\n",
    "    if word in glove_words:\n",
    "        embedding_vector = glove[word]\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        # Words not found in the embedding index will be all zeros\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "print(\"Generated embedding_matrix in {:.2f}\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the embedding matrix to file\n",
    "with open(\"embedding_matrix.pkl\",\"wb\") as f:\n",
    "    pickle.dump(embedding_matrix,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the tokenizer to file\n",
    "with open(\"token.pkl\",\"wb\") as f:\n",
    "    pickle.dump(token,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
