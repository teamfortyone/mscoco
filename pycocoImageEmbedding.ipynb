{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "from keras.applications.inception_v3 import InceptionV3,preprocess_input\n",
    "from keras.layers import Dense,BatchNormalization,Dropout,Embedding,RepeatVector\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "\n",
    "from pycocotools.coco import COCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDir='coco'\n",
    "dataType='train2014'\n",
    "annFile='{}/annotations/captions_{}.json'.format(dataDir,dataType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.93s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "coco=COCO(annFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we are using this as feature extractor, the last softmax layer is not useful for us.\n",
    "inception = InceptionV3(weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inception.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pop the last softmax layer and freezing the remaining layers\n",
    "inception.layers.pop()\n",
    "\n",
    "for layer in inception.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'mixed10/concat:0' shape=(None, 8, 8, 2048) dtype=float32>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inception.layers[-2].output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### image ==> [Inception + Embedding ] ==> Feature Extracted image of shape (None,300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/Kanishk/Documents/Projects/sem6/wsl/lib/python3.6/site-packages/ipykernel_launcher.py:2: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"av...)`\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# building the final model\n",
    "final_model = Model(input = inception.input,output = inception.layers[-1].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_SIZE = (299,299)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getImgPath(imgId):\n",
    "    padding = \"0\" * (12  - len(str(imgId)))\n",
    "    imgName = \"{}{}\".format(padding, imgId)\n",
    "    return \"{}/{}/COCO_{}_{}.jpg\".format(dataDir, dataType, dataType, imgName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'coco/train2014/COCO_train2014_000000001036.jpg'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getImgPath(1036)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for image imbedding i.e converting image to 300 dimentional\n",
    "train_image_extracted = dict()\n",
    "#imgIds = coco.getImgIds()\n",
    "\n",
    "imgIds = [151, 260, 307, 404, 450, 491, 514, 529, 575, 671] # dummy list because I don't have all images extracted\n",
    "\n",
    "try:\n",
    "    for imgId in imgIds:\n",
    "        img = load_img(getImgPath(imgId), target_size=TARGET_SIZE)\n",
    "        # Converting image to array\n",
    "        img_array = img_to_array(img)\n",
    "        nimage = preprocess_input(img_array)\n",
    "        # Adding one more dimesion\n",
    "        nimage = np.expand_dims(nimage, axis=0)    \n",
    "        fea_vec = final_model.predict(nimage)\n",
    "        train_image_extracted[imgId] = np.reshape(fea_vec, fea_vec.shape[1]) # reshape from (1, 2048) to (2048, )\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Exception got :- \\n\",e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{151: array([0.2930465 , 0.2663404 , 0.3843394 , ..., 0.14990748, 0.29548055,\n",
       "        0.20103858], dtype=float32),\n",
       " 260: array([1.5420294 , 0.23749   , 0.14261298, ..., 0.11536592, 0.9824455 ,\n",
       "        0.08451668], dtype=float32),\n",
       " 307: array([0.440467  , 0.5049137 , 0.147666  , ..., 0.22392224, 0.21844909,\n",
       "        0.2886581 ], dtype=float32),\n",
       " 404: array([0.23801179, 0.07262233, 0.3812816 , ..., 0.2133198 , 0.7977216 ,\n",
       "        0.03980841], dtype=float32),\n",
       " 450: array([0.00927397, 1.1330208 , 0.07134381, ..., 0.85767764, 0.6659479 ,\n",
       "        0.19884247], dtype=float32),\n",
       " 491: array([0.23473062, 0.26051185, 0.8018033 , ..., 0.28381902, 0.24237214,\n",
       "        0.2986067 ], dtype=float32),\n",
       " 514: array([0.2874124 , 0.20027205, 0.79848576, ..., 0.12744518, 0.0127114 ,\n",
       "        0.5092769 ], dtype=float32),\n",
       " 529: array([0.20294471, 0.7958152 , 0.24508199, ..., 0.3041519 , 0.39853734,\n",
       "        0.23544852], dtype=float32),\n",
       " 575: array([0.57837427, 0.6224444 , 0.01861988, ..., 0.37554434, 0.2275149 ,\n",
       "        0.08330914], dtype=float32),\n",
       " 671: array([0.17391978, 0.13964297, 0.09762084, ..., 0.13763534, 0.6034776 ,\n",
       "        0.87261707], dtype=float32)}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_image_extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_embedding_matrix(matrix):\n",
    "    with open(\"train_image_extracted.pkl\",\"wb\") as f:\n",
    "        pickle.dump(matrix, f)\n",
    "\n",
    "save_embedding_matrix(train_image_extracted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 2048)\n"
     ]
    }
   ],
   "source": [
    "print(final_model.output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
