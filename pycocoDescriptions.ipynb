{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "from pycocotools.coco import COCO\n",
    "from nltk import FreqDist\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decontracted(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_suffixes_and_prefixes(descriptions):\n",
    "    for k in descriptions.keys():\n",
    "        value = descriptions[k]\n",
    "        caption_list = []\n",
    "        for ec in value:\n",
    "\n",
    "            # replaces specific and general phrases\n",
    "            sent = decontracted(ec)\n",
    "            sent = sent.replace('\\\\r', ' ')\n",
    "            sent = sent.replace('\\\\\"', ' ')\n",
    "            sent = sent.replace('\\\\n', ' ')\n",
    "            sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n",
    "\n",
    "            # startseq is for kick starting the partial sequence generation and endseq is to stop while predicting.\n",
    "            # for more referance please check https://machinelearningmastery.com/develop-a-deep-learning-caption-generation-model-in-python/\n",
    "            image_cap = 'startseq ' + sent.lower() + ' endseq'\n",
    "            caption_list.append(image_cap)\n",
    "        descriptions[k] = caption_list\n",
    "    return descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDir='coco'\n",
    "dataType='train2014'\n",
    "annFile='{}/annotations/captions_{}.json'.format(dataDir,dataType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco=COCO(annFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annIds = coco.getAnnIds(imgIds=35783)\n",
    "anns = coco.loadAnns(annIds)\n",
    "coco.showAnns(anns)\n",
    "# print(type(coco.getImgIds()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions = {}\n",
    "imgIds = coco.getImgIds()\n",
    "# imgIds = [151, 260, 307, 404, 450, 491, 514, 529, 575, 671] # dummy list because I don't have all images extracted\n",
    "\n",
    "# print(len(imgIds))\n",
    "start = time.time()\n",
    "for imgId in imgIds:\n",
    "    annIds = coco.getAnnIds(imgIds=imgId)\n",
    "    # print(len(annIds))\n",
    "    anns = coco.loadAnns(annIds)\n",
    "    for annotation in anns:\n",
    "        if imgId in descriptions:\n",
    "            descriptions[imgId].append(annotation['caption'])\n",
    "        else:\n",
    "            descriptions[imgId] = list()\n",
    "            descriptions[imgId].append(annotation['caption'])\n",
    "print(\"Created Descriptions Dict in {:0.2f}s\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "descriptions = add_suffixes_and_prefixes(descriptions)\n",
    "print(\"Added suffixes and prefixes in {:0.2f}s\".format(time.time() - start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in descriptions.items():\n",
    "    print(v)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_descriptions(descriptions):\n",
    "    \"\"\"Dump processed captions into a pickle\"\"\"\n",
    "    with open(\"coco_descriptions.pkl\", \"wb\") as f:\n",
    "        pickle.dump(descriptions, f)\n",
    "\n",
    "def load_descriptions(file_path):\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_descriptions(descriptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note\n",
    "Go to `pycocoImageEmbedding` and find out how many corrupt images are present, and remove their captions from the above pickle before moving on.\n",
    "\n",
    "Here, we're using the `corruption_free_coco_descriptions.pkl` to generate stats about our datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_desc = load_descriptions(\"./corruption_free_coco_descriptions.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(new_desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(new_desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['startseq a restaurant has modern wooden tables and chairs  endseq', 'startseq a long restaurant table with rattan rounded back chairs  endseq', 'startseq a long table with a plant on top of it surrounded with wooden chairs  endseq', 'startseq a long table with a flower arrangement in the middle for meetings endseq', 'startseq a table is adorned with wooden chairs with blue accents  endseq']\n"
     ]
    }
   ],
   "source": [
    "for k, v in new_desc.items():\n",
    "    print(v)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Corpus in 0.31s\n",
      "The size of vocabulary is 23124\n"
     ]
    }
   ],
   "source": [
    "corpus = \"\"\n",
    "start = time.time()\n",
    "for ec in new_desc.values():\n",
    "    for el in ec:\n",
    "        corpus += \" \"+el\n",
    "print(\"Generated Corpus in {:.2f}s\".format(time.time() - start))\n",
    "\n",
    "total_words = corpus.split()\n",
    "vocabulary = set(total_words)\n",
    "print(\"The size of vocabulary is {}\".format(len(vocabulary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 684603),\n",
       " ('startseq', 414108),\n",
       " ('endseq', 414108),\n",
       " ('on', 150689),\n",
       " ('of', 142762)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating frequency distribution of words\n",
    "freq_dist = FreqDist(total_words)\n",
    "freq_dist.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing least common words from vocabulary\n",
    "for ew in list(vocabulary):\n",
    "    if(freq_dist[ew]<10):\n",
    "        vocabulary.remove(ew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique words after removing less frequent word from our corpus = 6321\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = len(vocabulary)+1\n",
    "print(\"Total unique words after removing less frequent word from our corpus = {}\".format(VOCAB_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total caption present = 414108\n"
     ]
    }
   ],
   "source": [
    "caption_list = []\n",
    "for el in new_desc.values():\n",
    "    for ec in el:\n",
    "        caption_list.append(ec)\n",
    "print(\"The total caption present = {}\".format(len(caption_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = Tokenizer(num_words=VOCAB_SIZE)\n",
    "token.fit_on_texts(caption_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index to words are assigned according to frequency. i.e the most frequent word has index of 1\n",
    "ix_to_word = token.index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in list(ix_to_word):\n",
    "    if k>=6321:\n",
    "        ix_to_word.pop(k, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_ix = dict()\n",
    "for k,v in ix_to_word.items():\n",
    "    word_to_ix[v] = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6320\n",
      "6320\n"
     ]
    }
   ],
   "source": [
    "print(len(word_to_ix))\n",
    "print(len(ix_to_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum caption has length of 52\n"
     ]
    }
   ],
   "source": [
    "# finding the max_length caption\n",
    "MAX_LENGTH = 0\n",
    "temp = 0\n",
    "for ec in caption_list:\n",
    "    temp = len(ec.split())\n",
    "    if(MAX_LENGTH<=temp):\n",
    "        MAX_LENGTH = temp\n",
    "\n",
    "print(\"Maximum caption has length of {}\".format(MAX_LENGTH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Glove Vectors file\n",
    "using 300 dimensions glove file since the article we're following uses a embedding size of 300\n",
    "\n",
    "Download pre-trained glove_vectors from [this link](https://nlp.stanford.edu/projects/glove/)\n",
    "\n",
    "We'll load the text file (`glove.6B.300d.txt`) and save it as a pickle (`glove_vectors.pkl`) to save us time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_glove_pickle(file_path):\n",
    "    embeddings_index = {}\n",
    "    start = time.time()\n",
    "    with open(file_path, encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "    print(\"Created embeddings_index in {:.2f}s\".format(time.time() - start))\n",
    "    \n",
    "    print(\"Saving embeddings_index as glove_vectors.pkl\")\n",
    "    start = time.time()\n",
    "    with open(\"glove_vectors.pkl\", \"wb\") as f:\n",
    "        pickle.dump(embeddings_index, f)\n",
    "    print(\"Saved glove_vectors in {:.2f}s\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created embeddings_index in 27.34s\n",
      "Saving embeddings_index as glove_vectors.pkl\n",
      "Saved glove_vectors in 35.15s\n"
     ]
    }
   ],
   "source": [
    "save_glove_pickle(\"glove/glove.6B.300d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_vectors(file_path):\n",
    "    start = time.time()\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        glove = pickle.load(f)\n",
    "        glove_words = set(glove.keys())\n",
    "    print(\"Loaded {} in {:.2f}s\".format(file_path, time.time() - start))\n",
    "    return glove, glove_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded glove_vectors.pkl in 2.03s\n"
     ]
    }
   ],
   "source": [
    "glove, glove_words = load_glove_vectors(\"glove_vectors.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(glove_words) # 400k\n",
    "# type(glove_words) # set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embedding_matrix in 0.49\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_SIZE = 300\n",
    "\n",
    "# Get 300-dim dense vector for each of the words in vocabulary\n",
    "embedding_matrix = np.zeros((VOCAB_SIZE,EMBEDDING_SIZE))\n",
    "embedding_matrix.shape\n",
    "\n",
    "start = time.time()\n",
    "for word, i in word_to_ix.items():\n",
    "    embedding_vector = np.zeros(300)\n",
    "    if word in glove_words:\n",
    "        embedding_vector = glove[word]\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        # Words not found in the embedding index will be all zeros\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "print(\"Generated embedding_matrix in {:.2f}\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the embedding matrix to file\n",
    "with open(\"embedding_matrix.pkl\",\"wb\") as f:\n",
    "    pickle.dump(embedding_matrix,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
